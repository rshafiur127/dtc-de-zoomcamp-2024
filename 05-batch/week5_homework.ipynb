{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13882b3-784d-4c12-bbf7-4525237842f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0616292f-208f-4ec2-a565-b6bbbe4605e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/05 10:09:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "\t.master(\"local[*]\") \\\n",
    "\t.appName('test') \\\n",
    "\t.config(\"spark.executor.memory\", \"4g\") \\\n",
    "\t.config(\"spark.driver.memory\", \"4g\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3976b3-1a3c-400d-96b6-1e278bf23d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferSchema = \"true\") \\\n",
    "    .csv('data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d31cfc8-7927-4838-a108-0f7e5bc928d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PUlocationID', IntegerType(), True), StructField('DOlocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8bda00-add9-4b39-8e1c-46b519e53226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num,pickup_datetime,dropOff_datetime,PUlocationID,DOlocationID,SR_Flag,Affiliated_base_number\n",
      "B00009,2019-10-01 00:23:00,2019-10-01 00:35:00,264,264,,B00009\n",
      "B00013,2019-10-01 00:11:29,2019-10-01 00:13:22,264,264,,B00013\n",
      "B00014,2019-10-01 00:11:43,2019-10-01 00:37:20,264,264,,B00014\n",
      "B00014,2019-10-01 00:56:29,2019-10-01 00:57:47,264,264,,B00014\n",
      "B00014,2019-10-01 00:23:09,2019-10-01 00:28:27,264,264,,B00014\n",
      "B00021         ,2019-10-01 00:00:48,2019-10-01 00:07:12,129,129,,B00021         \n",
      "B00021         ,2019-10-01 00:47:23,2019-10-01 00:53:25,57,57,,B00021         \n",
      "B00021         ,2019-10-01 00:10:06,2019-10-01 00:19:50,173,173,,B00021         \n",
      "B00021         ,2019-10-01 00:51:37,2019-10-01 01:06:14,226,226,,B00021         \n"
     ]
    }
   ],
   "source": [
    "!head data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8965c1d-02ae-4a14-8b7b-9f6e5fddf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb3fdd3-b42d-4cca-b2aa-2faf5c43d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "809fb83a-1a33-4e2c-b557-13a0ff9b1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7879c596-3421-41a3-92f5-12e261408423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde000ae-db9b-4a5a-8ded-38c03c1f910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26e82bfe-1f59-4bdd-a514-5b49187a0004",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pandas\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 517\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[0;32m-> 1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[1;32m   1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, _merge_type(f\u001b[38;5;241m.\u001b[39mdataType, nfs\u001b[38;5;241m.\u001b[39mget(f\u001b[38;5;241m.\u001b[39mname, NullType()), name\u001b[38;5;241m=\u001b[39mnew_name(f\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[1;32m   1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[0;32m-> 1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, \u001b[43m_merge_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNullType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/de-zoomcamp-2024/week5/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(b):\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not merge type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(a), \u001b[38;5;28mtype\u001b[39m(b))))\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# same type\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50db192a-bd46-4e0b-9265-dc7569ee768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de263b3a-999f-4cee-8093-a2a623bc00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "\ttypes.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2d31e48-465f-4db4-8f2a-e0cb67b618f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d1eab03-750e-4b5b-8c72-ddaec8a3a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c8ff2a6-49f5-42e9-8f2e-5d6615b55c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69f7e079-273c-4d28-bc4a-535388e17400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cbb6771-c7ac-4adf-b80d-a63906764c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ef68ec-0be5-495f-a682-f86223573b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00628', pickup_datetime=datetime.datetime(2019, 10, 4, 7, 6), dropoff_datetime=datetime.datetime(2019, 10, 4, 7, 38, 58), PULocationID=41, DOLocationID=265, SR_Flag=None, Affiliated_base_number='B02280'),\n",
       " Row(dispatching_base_num='B01639', pickup_datetime=datetime.datetime(2019, 10, 7, 16, 24, 31), dropoff_datetime=datetime.datetime(2019, 10, 7, 17, 2, 2), PULocationID=264, DOLocationID=127, SR_Flag=None, Affiliated_base_number='B01639'),\n",
       " Row(dispatching_base_num='B01315', pickup_datetime=datetime.datetime(2019, 10, 8, 13, 22, 9), dropoff_datetime=datetime.datetime(2019, 10, 8, 13, 47, 3), PULocationID=264, DOLocationID=159, SR_Flag=None, Affiliated_base_number='B01315'),\n",
       " Row(dispatching_base_num='B02401', pickup_datetime=datetime.datetime(2019, 10, 5, 13, 8, 16), dropoff_datetime=datetime.datetime(2019, 10, 5, 13, 23, 8), PULocationID=264, DOLocationID=17, SR_Flag=None, Affiliated_base_number='B02401'),\n",
       " Row(dispatching_base_num='B00111', pickup_datetime=datetime.datetime(2019, 10, 2, 21, 0, 6), dropoff_datetime=datetime.datetime(2019, 10, 2, 22, 4, 32), PULocationID=264, DOLocationID=265, SR_Flag=None, Affiliated_base_number='B00111'),\n",
       " Row(dispatching_base_num='B02293', pickup_datetime=datetime.datetime(2019, 10, 8, 16, 5, 55), dropoff_datetime=datetime.datetime(2019, 10, 8, 16, 24, 45), PULocationID=62, DOLocationID=33, SR_Flag=None, Affiliated_base_number='B02293'),\n",
       " Row(dispatching_base_num='B02634', pickup_datetime=datetime.datetime(2019, 10, 7, 8, 54, 17), dropoff_datetime=datetime.datetime(2019, 10, 7, 9, 7), PULocationID=264, DOLocationID=242, SR_Flag=None, Affiliated_base_number='B02634'),\n",
       " Row(dispatching_base_num='B01051', pickup_datetime=datetime.datetime(2019, 10, 6, 17, 42, 16), dropoff_datetime=datetime.datetime(2019, 10, 6, 17, 54, 19), PULocationID=264, DOLocationID=119, SR_Flag=None, Affiliated_base_number='B01051'),\n",
       " Row(dispatching_base_num='B00937', pickup_datetime=datetime.datetime(2019, 10, 4, 0, 42, 24), dropoff_datetime=datetime.datetime(2019, 10, 4, 1, 8, 10), PULocationID=264, DOLocationID=241, SR_Flag=None, Affiliated_base_number='B00937'),\n",
       " Row(dispatching_base_num='B01280', pickup_datetime=datetime.datetime(2019, 10, 5, 7, 28), dropoff_datetime=datetime.datetime(2019, 10, 5, 7, 52), PULocationID=264, DOLocationID=264, SR_Flag=None, Affiliated_base_number='B01280')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abdd5312-0265-42bd-acdd-ae8658007db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d7a2b53-8887-40e8-b8be-57458a8186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00cfa58a-68bb-4baa-8a41-813fb6cdc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00254|2019-10-15 21:06:47|2019-10-15 21:17:43|          13|          68|   null|                B00254|\n",
      "|              B00111|2019-10-15 06:18:17|2019-10-15 07:07:06|         264|         265|   null|                B00111|\n",
      "|              B02849|2019-10-15 12:28:45|2019-10-15 12:59:40|         264|         145|   null|                B02849|\n",
      "|              B01437|2019-10-15 11:46:44|2019-10-15 11:54:34|         264|         197|   null|                B01437|\n",
      "|              B01437|2019-10-15 13:00:41|2019-10-15 13:35:06|         264|          82|   null|                B02880|\n",
      "|              B02111|2019-10-15 22:43:06|2019-10-15 22:49:03|          92|          92|   null|                B02111|\n",
      "|              B02243|2019-10-15 09:27:00|2019-10-15 10:10:00|          63|         225|   null|                LX-209|\n",
      "|              B00227|2019-10-15 22:13:53|2019-10-15 23:03:00|         264|         264|   null|                B00227|\n",
      "|              B00919|2019-10-15 07:42:58|2019-10-15 07:45:00|         264|          41|   null|                B00919|\n",
      "|              B00628|2019-10-15 20:38:47|2019-10-15 20:58:49|          88|         186|   null|                B00628|\n",
      "|              B02998|2019-10-15 08:33:59|2019-10-15 08:44:48|         168|         168|   null|                B02998|\n",
      "|              B01087|2019-10-15 21:42:21|2019-10-15 22:14:35|         234|         265|   null|                B01087|\n",
      "|              B02881|2019-10-15 11:26:17|2019-10-15 12:07:59|           7|         216|   null|                B02881|\n",
      "|              B02509|2019-10-15 11:09:00|2019-10-15 11:49:56|         264|          21|   null|                B02879|\n",
      "|              B00271|2019-10-15 05:45:37|2019-10-15 05:55:47|         264|         194|   null|                B02867|\n",
      "|              B02849|2019-10-15 06:51:37|2019-10-15 06:56:20|         264|         100|   null|                B02849|\n",
      "|              B02784|2019-10-15 14:13:13|2019-10-15 15:05:13|          86|          92|   null|                  null|\n",
      "|              B00310|2019-10-15 11:21:55|2019-10-15 11:27:11|         264|         126|   null|                B02878|\n",
      "|              B01717|2019-10-15 13:58:19|2019-10-15 14:17:51|         264|          42|   null|                B01717|\n",
      "|              B00419|2019-10-15 01:09:52|2019-10-15 01:12:24|         184|          51|   null|                B00419|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "*\n",
    "FROM\n",
    "    fhv_data\n",
    "where cast(pickup_datetime as date) = '2019-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "271bc221-d81b-491e-a2d9-b8c6f7e6dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|dispatching_base_num|       trip_distance|\n",
      "+--------------------+--------------------+\n",
      "|              B02832|INTERVAL '26298 0...|\n",
      "|              B02416|INTERVAL '3653 00...|\n",
      "|     B00746         |INTERVAL '2922 00...|\n",
      "|              B02921|INTERVAL '366 10:...|\n",
      "|              B03110|INTERVAL '366 00:...|\n",
      "|              B03080|INTERVAL '61 00:3...|\n",
      "|     B03084         |INTERVAL '44 00:4...|\n",
      "|              B01452|INTERVAL '33 01:3...|\n",
      "|              B00972|INTERVAL '33 00:4...|\n",
      "|              B02418|INTERVAL '31 01:3...|\n",
      "|              B01455|INTERVAL '31 00:1...|\n",
      "|              B02732|INTERVAL '25 04:0...|\n",
      "|              B02532|INTERVAL '19 13:4...|\n",
      "|              B02546|INTERVAL '18 00:4...|\n",
      "|              B01985|INTERVAL '16 14:5...|\n",
      "|              B03184|INTERVAL '14 19:0...|\n",
      "|              B03107|INTERVAL '11 02:1...|\n",
      "|              B02037|INTERVAL '10 00:1...|\n",
      "|              B00272|INTERVAL '7 00:14...|\n",
      "|              B02067|INTERVAL '6 23:20...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "dispatching_base_num,\n",
    "max(dropoff_datetime - pickup_datetime) as trip_distance \n",
    "FROM\n",
    "    fhv_data\n",
    "group by dispatching_base_num\n",
    "order by trip_distance desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5a422c7-8df8-4fd5-a522-38dd467f108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     trip_distance|\n",
      "+------------------+\n",
      "|          631152.5|\n",
      "|          631152.5|\n",
      "| 87672.44083333333|\n",
      "| 70128.02805555555|\n",
      "|            8794.0|\n",
      "| 8784.166666666666|\n",
      "|1464.5344444444445|\n",
      "|1056.8266666666666|\n",
      "|1056.2705555555556|\n",
      "| 793.5530555555556|\n",
      "| 793.3858333333334|\n",
      "|          793.2975|\n",
      "| 792.9980555555555|\n",
      "| 792.9883333333333|\n",
      "| 792.8602777777778|\n",
      "| 792.8108333333333|\n",
      "|           792.785|\n",
      "| 792.7694444444444|\n",
      "| 792.7538888888889|\n",
      "| 792.7463888888889|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "(unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime))/3600 as trip_distance \n",
    "FROM\n",
    "    fhv_data\n",
    "order by trip_distance desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57aa93f2-10c2-4d53-81a2-fd2a1977435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|PULocationID|count(1)|\n",
      "+------------+--------+\n",
      "|           2|       1|\n",
      "|         105|       2|\n",
      "|         111|       5|\n",
      "|          30|       8|\n",
      "|         120|      14|\n",
      "|          12|      15|\n",
      "|         207|      23|\n",
      "|          27|      25|\n",
      "|         154|      26|\n",
      "|           8|      29|\n",
      "|         128|      39|\n",
      "|         253|      47|\n",
      "|          96|      53|\n",
      "|          34|      57|\n",
      "|          59|      62|\n",
      "|          58|      77|\n",
      "|          99|      89|\n",
      "|         190|      98|\n",
      "|          54|     105|\n",
      "|         217|     110|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    " PULocationID, count(1)\n",
    "FROM\n",
    "    fhv_data\n",
    "GROUP BY PULocationID\n",
    "ORDER BY 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80f60285-0b41-403a-870b-9e9d303d6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-05 14:09:03--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.169.32, 52.217.88.206, 52.217.116.144, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.169.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-03-05 14:09:05 (148 KB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3968948-1eb3-45dc-b11d-3ffcd6fb4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f222cb86-5d46-4ed7-ac1e-0e1fa30c5b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zone.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fae9fb9b-78d1-416c-97fc-1a391afca87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d179fb2-a9e3-4e0c-810d-b0ce29f74cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fffb7006-2411-4adc-9e24-12874b0c21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33f5695c-c5fe-4aa2-a665-85fb1b6c9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc7b9f58-c55e-487c-979e-cd0dc0e6ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a972e35-fc60-4c6f-b156-2dd30ce119fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID=1, Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID=2, Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID=3, Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone'),\n",
       " Row(LocationID=4, Borough='Manhattan', Zone='Alphabet City', service_zone='Yellow Zone'),\n",
       " Row(LocationID=5, Borough='Staten Island', Zone='Arden Heights', service_zone='Boro Zone'),\n",
       " Row(LocationID=6, Borough='Staten Island', Zone='Arrochar/Fort Wadsworth', service_zone='Boro Zone'),\n",
       " Row(LocationID=7, Borough='Queens', Zone='Astoria', service_zone='Boro Zone'),\n",
       " Row(LocationID=8, Borough='Queens', Zone='Astoria Park', service_zone='Boro Zone'),\n",
       " Row(LocationID=9, Borough='Queens', Zone='Auburndale', service_zone='Boro Zone'),\n",
       " Row(LocationID=10, Borough='Queens', Zone='Baisley Park', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zone.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88fb7618-1d6b-418f-98d4-5b49e1127954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7692418-9f70-4eee-87c8-02cddd019648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "*\n",
    "FROM\n",
    "    zones\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "301d3434-4470-46dd-a6b2-644e96d0bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+\n",
      "|PULocationID|                zone|count(1)|\n",
      "+------------+--------------------+--------+\n",
      "|           2|         Jamaica Bay|       1|\n",
      "|         105|Governor's Island...|       2|\n",
      "|         111| Green-Wood Cemetery|       5|\n",
      "|          30|       Broad Channel|       8|\n",
      "|         120|     Highbridge Park|      14|\n",
      "|          12|        Battery Park|      15|\n",
      "|         207|Saint Michaels Ce...|      23|\n",
      "|          27|Breezy Point/Fort...|      25|\n",
      "|         154|Marine Park/Floyd...|      26|\n",
      "|           8|        Astoria Park|      29|\n",
      "|         128|    Inwood Hill Park|      39|\n",
      "|         253|       Willets Point|      47|\n",
      "|          96|Forest Park/Highl...|      53|\n",
      "|          34|  Brooklyn Navy Yard|      57|\n",
      "|          59|        Crotona Park|      62|\n",
      "|          58|        Country Club|      77|\n",
      "|          99|     Freshkills Park|      89|\n",
      "|         190|       Prospect Park|      98|\n",
      "|          54|     Columbia Street|     105|\n",
      "|         217|  South Williamsburg|     110|\n",
      "+------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "\tfhv.PULocationID, z.zone, count(1) \n",
    "FROM\n",
    "    fhv_data fhv, zones z\n",
    "where fhv.PULocationID = z.LocationID\n",
    "GROUP BY fhv.PULocationID, z.zone\n",
    "ORDER BY 3\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
